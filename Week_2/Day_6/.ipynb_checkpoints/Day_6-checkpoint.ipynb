{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 6: Linear Regression\n",
        "\n",
        "**Course**: Machine Learning with Python: From Basics to Applications  \n",
        "**Objective**: Understand linear regression and apply it to predict house prices using the Boston Housing dataset.  \n",
        "**Prerequisites**: Basic Python, NumPy, Pandas (Day 2), preprocessing (Days 3–5).  \n",
        "**Tools**: Pandas, scikit-learn (install with `pip install pandas scikit-learn`).  \n",
        "**Dataset**: Boston Housing dataset (available via `sklearn.datasets.load_boston`).  \n",
        "\n",
        "In this notebook, we will:  \n",
        "1. Load the Boston Housing dataset.  \n",
        "2. Split data into training (80%) and test (20%) sets.  \n",
        "3. Scale features using `StandardScaler`.  \n",
        "4. Train a linear regression model and predict on the test set.  \n",
        "5. Evaluate with Mean Squared Error (MSE) and R-squared (R²).  \n",
        "6. Save predictions to a CSV.  \n",
        "7. Verify splits, scaling, and metrics.  \n",
        "\n",
        "Let’s get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Import Libraries\n",
        "\n",
        "Import Pandas for data handling and scikit-learn for dataset loading, splitting, scaling, modeling, and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load the Boston Housing Dataset\n",
        "\n",
        "Load the dataset and convert it to a Pandas DataFrame for easier inspection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "boston = load_boston()\n",
        "X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
        "y = boston.target\n",
        "\n",
        "# Display first 5 rows\n",
        "print(\"First 5 rows of features:\")\n",
        "print(X.head())\n",
        "\n",
        "# Display dataset info\n",
        "print(\"\\nDataset shape:\", X.shape)\n",
        "print(\"Target shape:\", y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Expected Output**:  \n",
        "- `X.head()` shows 13 features (e.g., `CRIM`, `RM`, `AGE`).  \n",
        "- `X.shape`: (506, 13), `y.shape`: (506,).  \n",
        "\n",
        "**Note**: The dataset has no missing values, so no imputation is needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Train/Test Split\n",
        "\n",
        "Split the data into 80% training and 20% test sets with `random_state=42`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Verify shapes\n",
        "print(\"Training set shape (X_train, y_train):\", X_train.shape, y_train.shape)\n",
        "print(\"Test set shape (X_test, y_test):\", X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Expected Output**:  \n",
        "- Training: ~404 rows, test: ~102 rows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Feature Scaling\n",
        "\n",
        "Scale all features using `StandardScaler` to ensure linear regression performs well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform training data\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_train = pd.DataFrame(X_train, columns=boston.feature_names)\n",
        "\n",
        "# Transform test data\n",
        "X_test = scaler.transform(X_test)\n",
        "X_test = pd.DataFrame(X_test, columns=boston.feature_names)\n",
        "\n",
        "# Verify scaling\n",
        "print(\"Training set mean (first few features):\")\n",
        "print(X_train.mean()[:5])\n",
        "print(\"\\nTraining set std (first few features):\")\n",
        "print(X_train.std()[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Expected Output**:  \n",
        "- Mean ~0, std ~1 for all features in `X_train`.  \n",
        "- `X_test` stats may differ slightly (normal, as scaler was fit on training data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Train Linear Regression Model\n",
        "\n",
        "Train the model on the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize and train model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Predict and Evaluate\n",
        "\n",
        "Predict on the test set and compute MSE and R²."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"R-squared (R²):\", r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Expected Output**:  \n",
        "- MSE: ~20–30 (depends on split).  \n",
        "- R²: ~0.7–0.8 (indicates good fit)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Save Predictions\n",
        "\n",
        "Save actual and predicted values to a CSV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create DataFrame with actual and predicted values\n",
        "results = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
        "results.to_csv('boston_predictions.csv', index=False)\n",
        "\n",
        "print(\"Predictions saved as boston_predictions.csv\")\n",
        "print(\"\\nFirst 5 predictions:\")\n",
        "print(results.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Verification\n",
        "\n",
        "Verify split sizes, scaling, and metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify split sizes\n",
        "print(\"Training set size:\", X_train.shape[0])\n",
        "print(\"Test set size:\", X_test.shape[0])\n",
        "\n",
        "# Verify scaling\n",
        "print(\"\\nTraining set mean (first feature):\", X_train.iloc[:, 0].mean())\n",
        "print(\"Training set std (first feature):\", X_train.iloc[:, 0].std())\n",
        "\n",
        "# Verify metrics\n",
        "print(\"\\nMSE:\", mse)\n",
        "print(\"R²:\", r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Expected Output**:  \n",
        "- Training: ~404 rows, test: ~102 rows.  \n",
        "- Mean ~0, std ~1 for training features.  \n",
        "- Reasonable MSE and R² values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assignment\n",
        "\n",
        "1. Run this notebook to train and evaluate a linear regression model on the Boston Housing dataset.  \n",
        "2. Verify:  \n",
        "   - Split sizes (~404 train, ~102 test).  \n",
        "   - Features scaled (mean ~0, std ~1 in training data).  \n",
        "   - Reasonable MSE (~20–30) and R² (~0.7–0.8).  \n",
        "3. Save predictions to `boston_predictions.csv`.  \n",
        "4. Submit a screenshot of the notebook output showing split sizes, MSE, and R².  \n",
        "\n",
        "**Next Steps**: On Day 7, we’ll explore logistic regression for classification using the Titanic dataset."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}