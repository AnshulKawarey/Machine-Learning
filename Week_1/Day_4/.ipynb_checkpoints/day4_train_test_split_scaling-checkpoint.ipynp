{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 4: Train/Test Split and Feature Scaling\n",
        "\n",
        "**Course**: Machine Learning with Python: From Basics to Applications  \n",
        "**Objective**: Learn to split data into training and test sets and apply feature scaling to numerical features to prepare for machine learning models.  \n",
        "**Prerequisites**: Preprocessed Titanic dataset (`titanic_preprocessed.csv`) from Day 3, with missing values handled and categorical variables encoded.  \n",
        "**Tools**: Pandas, scikit-learn (install with `pip install pandas scikit-learn`).  \n",
        "**Dataset**: Titanic dataset (preprocessed, available from Day 3 or Kaggle).  \n",
        "\n",
        "In this notebook, we will:  \n",
        "1. Load the preprocessed Titanic dataset.  \n",
        "2. Split data into training (80%) and test (20%) sets using `train_test_split`.  \n",
        "3. Apply `StandardScaler` to scale numerical features (`Age`, `Fare`).  \n",
        "4. Save the resulting datasets as CSVs for future use.  \n",
        "5. Verify the splits and scaling.  \n",
        "\n",
        "Let’s get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Import Libraries\n",
        "\n",
        "We need Pandas for data handling and scikit-learn for splitting and scaling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load Preprocessed Data\n",
        "\n",
        "Load the preprocessed Titanic dataset from Day 3 (`titanic_preprocessed.csv`). This dataset should have no missing values, with `Sex` encoded as 0/1 and `Embarked` as one-hot encoded columns (`Embarked_C`, `Embarked_Q`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the preprocessed dataset\n",
        "df = pd.read_csv('titanic_preprocessed.csv')\n",
        "\n",
        "# Display first 5 rows to confirm\n",
        "print(\"First 5 rows of preprocessed data:\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Separate Features and Target\n",
        "\n",
        "Separate the dataset into features (`X`) and target (`y`). The target is `Survived`, and all other columns are features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Features (drop 'Survived')\n",
        "X = df.drop('Survived', axis=1)\n",
        "\n",
        "# Target\n",
        "y = df['Survived']\n",
        "\n",
        "# Verify shapes\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Train/Test Split\n",
        "\n",
        "Split the data into 80% training and 20% test sets using `train_test_split`. Set `random_state=42` for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Verify shapes\n",
        "print(\"Training set shape (X_train, y_train):\", X_train.shape, y_train.shape)\n",
        "print(\"Test set shape (X_test, y_test):\", X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Feature Scaling\n",
        "\n",
        "Apply `StandardScaler` to scale numerical features (`Age`, `Fare`). Fit the scaler on the training data and transform both training and test data to avoid data leakage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform on training data (Age, Fare)\n",
        "X_train[['Age', 'Fare']] = scaler.fit_transform(X_train[['Age', 'Fare']])\n",
        "\n",
        "# Transform test data (using same scaler)\n",
        "X_test[['Age', 'Fare']] = scaler.transform(X_test[['Age', 'Fare']])\n",
        "\n",
        "# Verify scaling (mean ~0, std ~1 for training data)\n",
        "print(\"Training Age mean after scaling:\", X_train['Age'].mean())\n",
        "print(\"Training Age std after scaling:\", X_train['Age'].std())\n",
        "print(\"Training Fare mean after scaling:\", X_train['Fare'].mean())\n",
        "print(\"Training Fare std after scaling:\", X_train['Fare'].std())\n",
        "\n",
        "# Display first few rows of scaled training data\n",
        "print(\"\\nFirst 5 rows of X_train after scaling:\")\n",
        "X_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Save Datasets\n",
        "\n",
        "Save the split and scaled datasets as CSV files for use in future sessions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save training and test datasets\n",
        "X_train.to_csv('titanic_X_train.csv', index=False)\n",
        "X_test.to_csv('titanic_X_test.csv', index=False)\n",
        "y_train.to_csv('titanic_y_train.csv', index=False)\n",
        "y_test.to_csv('titanic_y_test.csv', index=False)\n",
        "\n",
        "print(\"Datasets saved as titanic_X_train.csv, titanic_X_test.csv, titanic_y_train.csv, titanic_y_test.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Verification\n",
        "\n",
        "Check that the datasets were saved correctly and that scaling worked as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify saved files\n",
        "print(\"\\nVerify X_train.csv:\")\n",
        "print(pd.read_csv('titanic_X_train.csv').head())\n",
        "\n",
        "# Confirm no missing values\n",
        "print(\"\\nMissing values in X_train:\", X_train.isnull().sum().sum())\n",
        "print(\"Missing values in X_test:\", X_test.isnull().sum().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assignment\n",
        "\n",
        "1. Run this notebook to split and scale the Titanic dataset.  \n",
        "2. Verify the shapes of `X_train`, `X_test`, `y_train`, and `y_test` (should be ~712 for training, ~179 for test).  \n",
        "3. Confirm that `Age` and `Fare` in `X_train` have mean ~0 and std ~1 after scaling.  \n",
        "4. Ensure the saved CSV files (`titanic_X_train.csv`, etc.) are created and contain the expected data.  \n",
        "5. Submit a screenshot of the notebook output showing the shapes and scaling statistics.  \n",
        "\n",
        "**Next Steps**: On Day 5, you’ll combine all preprocessing steps (Days 3-4) to prepare the Titanic dataset for modeling."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}