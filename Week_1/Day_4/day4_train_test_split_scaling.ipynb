{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4: Train/Test Split and Feature Scaling\n",
    "\n",
    "**Course**: Machine Learning with Python: From Basics to Applications  \n",
    "**Objective**: Learn to split data into training and test sets and apply feature scaling to numerical features to prepare for machine learning models.  \n",
    "**Prerequisites**: Preprocessed Titanic dataset (`titanic_preprocessed.csv`) from Day 3, with missing values handled and categorical variables encoded.  \n",
    "**Tools**: Pandas, scikit-learn (install with `pip install pandas scikit-learn`).  \n",
    "**Dataset**: Titanic dataset (preprocessed, available from Day 3 or Kaggle).  \n",
    "\n",
    "In this notebook, we will:  \n",
    "1. Load the preprocessed Titanic dataset.  \n",
    "2. Split data into training (80%) and test (20%) sets using `train_test_split`.  \n",
    "3. Apply `StandardScaler` to scale numerical features (`Age`, `Fare`).  \n",
    "4. Save the resulting datasets as CSVs for future use.  \n",
    "5. Verify the splits and scaling.  \n",
    "\n",
    "Let’s get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries\n",
    "\n",
    "We need Pandas for data handling and scikit-learn for splitting and scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Preprocessed Data\n",
    "\n",
    "Load the preprocessed Titanic dataset from Day 3 (`titanic_preprocessed.csv`). This dataset should have no missing values, with `Sex` encoded as 0/1 and `Embarked` as one-hot encoded columns (`Embarked_C`, `Embarked_Q`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of preprocessed data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name  Sex   Age  SibSp  Parch  \\\n",
       "0                            Braund, Mr. Owen Harris    1  22.0      1      0   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...    0  38.0      1      0   \n",
       "2                             Heikkinen, Miss. Laina    0  26.0      0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)    0  35.0      1      0   \n",
       "4                           Allen, Mr. William Henry    1  35.0      0      0   \n",
       "\n",
       "             Ticket     Fare Cabin  Embarked_Q  Embarked_S  \n",
       "0         A/5 21171   7.2500   NaN       False        True  \n",
       "1          PC 17599  71.2833   C85       False       False  \n",
       "2  STON/O2. 3101282   7.9250   NaN       False        True  \n",
       "3            113803  53.1000  C123       False        True  \n",
       "4            373450   8.0500   NaN       False        True  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the preprocessed dataset\n",
    "df = pd.read_csv('titanic_preprocessed.csv')\n",
    "\n",
    "# Display first 5 rows to confirm\n",
    "print(\"First 5 rows of preprocessed data:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Separate Features and Target\n",
    "\n",
    "Separate the dataset into features (`X`) and target (`y`). The target is `Survived`, and all other columns are features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features (drop 'Survived')\n",
    "X = df.drop('Survived', axis=1)\n",
    "\n",
    "# Target\n",
    "y = df['Survived']\n",
    "\n",
    "# Verify shapes\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train/Test Split\n",
    "\n",
    "Split the data into 80% training and 20% test sets using `train_test_split`. Set `random_state=42` for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Verify shapes\n",
    "print(\"Training set shape (X_train, y_train):\", X_train.shape, y_train.shape)\n",
    "print(\"Test set shape (X_test, y_test):\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Feature Scaling\n",
    "\n",
    "Apply `StandardScaler` to scale numerical features (`Age`, `Fare`). Fit the scaler on the training data and transform both training and test data to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform on training data (Age, Fare)\n",
    "X_train[['Age', 'Fare']] = scaler.fit_transform(X_train[['Age', 'Fare']])\n",
    "\n",
    "# Transform test data (using same scaler)\n",
    "X_test[['Age', 'Fare']] = scaler.transform(X_test[['Age', 'Fare']])\n",
    "\n",
    "# Verify scaling (mean ~0, std ~1 for training data)\n",
    "print(\"Training Age mean after scaling:\", X_train['Age'].mean())\n",
    "print(\"Training Age std after scaling:\", X_train['Age'].std())\n",
    "print(\"Training Fare mean after scaling:\", X_train['Fare'].mean())\n",
    "print(\"Training Fare std after scaling:\", X_train['Fare'].std())\n",
    "\n",
    "# Display first few rows of scaled training data\n",
    "print(\"\\nFirst 5 rows of X_train after scaling:\")\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Save Datasets\n",
    "\n",
    "Save the split and scaled datasets as CSV files for use in future sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training and test datasets\n",
    "X_train.to_csv('titanic_X_train.csv', index=False)\n",
    "X_test.to_csv('titanic_X_test.csv', index=False)\n",
    "y_train.to_csv('titanic_y_train.csv', index=False)\n",
    "y_test.to_csv('titanic_y_test.csv', index=False)\n",
    "\n",
    "print(\"Datasets saved as titanic_X_train.csv, titanic_X_test.csv, titanic_y_train.csv, titanic_y_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Verification\n",
    "\n",
    "Check that the datasets were saved correctly and that scaling worked as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify saved files\n",
    "print(\"\\nVerify X_train.csv:\")\n",
    "print(pd.read_csv('titanic_X_train.csv').head())\n",
    "\n",
    "# Confirm no missing values\n",
    "print(\"\\nMissing values in X_train:\", X_train.isnull().sum().sum())\n",
    "print(\"Missing values in X_test:\", X_test.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "1. Run this notebook to split and scale the Titanic dataset.  \n",
    "2. Verify the shapes of `X_train`, `X_test`, `y_train`, and `y_test` (should be ~712 for training, ~179 for test).  \n",
    "3. Confirm that `Age` and `Fare` in `X_train` have mean ~0 and std ~1 after scaling.  \n",
    "4. Ensure the saved CSV files (`titanic_X_train.csv`, etc.) are created and contain the expected data.  \n",
    "5. Submit a screenshot of the notebook output showing the shapes and scaling statistics.  \n",
    "\n",
    "**Next Steps**: On Day 5, you’ll combine all preprocessing steps (Days 3-4) to prepare the Titanic dataset for modeling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
